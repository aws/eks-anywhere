apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: test
  name: test
  namespace: eksa-system
spec:
  clusterNetwork:
    pods:
      cidrBlocks: [192.168.0.0/16]
    services:
      cidrBlocks: [10.96.0.0/12]
  controlPlaneEndpoint:
    host: 1.2.3.4
    port: 6443
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: test
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: TinkerbellCluster
    name: test
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: test
  namespace: eksa-system
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      imageRepository: public.ecr.aws/eks-distro/kubernetes
      etcd:
        local:
          imageRepository: public.ecr.aws/eks-distro/etcd-io
          imageTag: v3.4.16-eks-1-21-4
      dns:
        imageRepository: public.ecr.aws/eks-distro/coredns
        imageTag: v1.8.3-eks-1-21-4
      apiServer:
        extraArgs:
          audit-policy-file: /etc/kubernetes/audit-policy.yaml
          audit-log-path: /var/log/kubernetes/api-audit.log
          audit-log-maxage: "30"
          audit-log-maxbackup: "10"
          audit-log-maxsize: "512"
          feature-gates: ServiceLoadBalancerClass=true
          oidc-client-id: my-client-id
          oidc-groups-claim: claim1
          oidc-groups-prefix: prefix-for-groups
          oidc-issuer-url: https://mydomain.com/issuer
          oidc-required-claim: sub=test
          oidc-username-claim: username-claim
          oidc-username-prefix: username-prefix1
        extraVolumes:
        - hostPath: /etc/kubernetes/audit-policy.yaml
          mountPath: /etc/kubernetes/audit-policy.yaml
          name: audit-policy
          pathType: File
          readOnly: true
        - hostPath: /var/log/kubernetes
          mountPath: /var/log/kubernetes
          name: audit-log-dir
          pathType: DirectoryOrCreate
          readOnly: false
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          provider-id: PROVIDER_ID
          read-only-port: "0"
          anonymous-auth: "false"
          tls-cipher-suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
    joinConfiguration:
      nodeRegistration:
        ignorePreflightErrors:
        - DirAvailable--etc-kubernetes-manifests
        kubeletExtraArgs:
          provider-id: PROVIDER_ID
          read-only-port: "0"
          anonymous-auth: "false"
          tls-cipher-suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
    files:
      - content: |
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            name: kube-vip
            namespace: kube-system
          spec:
            containers:
            - args:
              - manager
              env:
              - name: vip_arp
                value: "true"
              - name: port
                value: "6443"
              - name: vip_cidr
                value: "32"
              - name: cp_enable
                value: "true"
              - name: cp_namespace
                value: kube-system
              - name: vip_ddns
                value: "false"
              - name: vip_leaderelection
                value: "true"
              - name: vip_leaseduration
                value: "15"
              - name: vip_renewdeadline
                value: "10"
              - name: vip_retryperiod
                value: "2"
              - name: address
                value: 1.2.3.4
              image: public.ecr.aws/l0g8r8j6/kube-vip/kube-vip:v0.3.7-eks-a-v0.0.0-dev-build.581
              imagePullPolicy: IfNotPresent
              name: kube-vip
              resources: {}
              securityContext:
                capabilities:
                  add:
                  - NET_ADMIN
                  - NET_RAW
              volumeMounts:
              - mountPath: /etc/kubernetes/admin.conf
                name: kubeconfig
            hostNetwork: true
            volumes:
            - hostPath:
                path: /etc/kubernetes/admin.conf
              name: kubeconfig
          status: {}
        owner: root:root
        path: /etc/kubernetes/manifests/kube-vip.yaml
      - content: |
          apiVersion: audit.k8s.io/v1beta1
          kind: Policy
          rules:
          # Log aws-auth configmap changes
          - level: RequestResponse
            namespaces: ["kube-system"]
            verbs: ["update", "patch", "delete"]
            resources:
            - group: "" # core
              resources: ["configmaps"]
              resourceNames: ["aws-auth"]
            omitStages:
            - "RequestReceived"
          # The following requests were manually identified as high-volume and low-risk,
          # so drop them.
          - level: None
            users: ["system:kube-proxy"]
            verbs: ["watch"]
            resources:
            - group: "" # core
              resources: ["endpoints", "services", "services/status"]
          - level: None
            users: ["kubelet"] # legacy kubelet identity
            verbs: ["get"]
            resources:
            - group: "" # core
              resources: ["nodes", "nodes/status"]
          - level: None
            userGroups: ["system:nodes"]
            verbs: ["get"]
            resources:
            - group: "" # core
              resources: ["nodes", "nodes/status"]
          - level: None
            users:
            - system:kube-controller-manager
            - system:kube-scheduler
            - system:serviceaccount:kube-system:endpoint-controller
            verbs: ["get", "update"]
            namespaces: ["kube-system"]
            resources:
            - group: "" # core
              resources: ["endpoints"]
          - level: None
            users: ["system:apiserver"]
            verbs: ["get"]
            resources:
            - group: "" # core
              resources: ["namespaces", "namespaces/status", "namespaces/finalize"]
          # Don't log HPA fetching metrics.
          - level: None
            users:
            - system:kube-controller-manager
            verbs: ["get", "list"]
            resources:
            - group: "metrics.k8s.io"
          # Don't log these read-only URLs.
          - level: None
            nonResourceURLs:
            - /healthz*
            - /version
            - /swagger*
          # Don't log events requests.
          - level: None
            resources:
            - group: "" # core
              resources: ["events"]
          # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes
          - level: Request
            users: ["kubelet", "system:node-problem-detector", "system:serviceaccount:kube-system:node-problem-detector"]
            verbs: ["update","patch"]
            resources:
            - group: "" # core
              resources: ["nodes/status", "pods/status"]
            omitStages:
            - "RequestReceived"
          - level: Request
            userGroups: ["system:nodes"]
            verbs: ["update","patch"]
            resources:
            - group: "" # core
              resources: ["nodes/status", "pods/status"]
            omitStages:
            - "RequestReceived"
          # deletecollection calls can be large, don't log responses for expected namespace deletions
          - level: Request
            users: ["system:serviceaccount:kube-system:namespace-controller"]
            verbs: ["deletecollection"]
            omitStages:
            - "RequestReceived"
          # Secrets, ConfigMaps, and TokenReviews can contain sensitive & binary data,
          # so only log at the Metadata level.
          - level: Metadata
            resources:
            - group: "" # core
              resources: ["secrets", "configmaps"]
            - group: authentication.k8s.io
              resources: ["tokenreviews"]
            omitStages:
              - "RequestReceived"
          - level: Request
            resources:
            - group: ""
              resources: ["serviceaccounts/token"]
          # Get repsonses can be large; skip them.
          - level: Request
            verbs: ["get", "list", "watch"]
            resources:
            - group: "" # core
            - group: "admissionregistration.k8s.io"
            - group: "apiextensions.k8s.io"
            - group: "apiregistration.k8s.io"
            - group: "apps"
            - group: "authentication.k8s.io"
            - group: "authorization.k8s.io"
            - group: "autoscaling"
            - group: "batch"
            - group: "certificates.k8s.io"
            - group: "extensions"
            - group: "metrics.k8s.io"
            - group: "networking.k8s.io"
            - group: "policy"
            - group: "rbac.authorization.k8s.io"
            - group: "scheduling.k8s.io"
            - group: "settings.k8s.io"
            - group: "storage.k8s.io"
            omitStages:
            - "RequestReceived"
          # Default level for known APIs
          - level: RequestResponse
            resources:
            - group: "" # core
            - group: "admissionregistration.k8s.io"
            - group: "apiextensions.k8s.io"
            - group: "apiregistration.k8s.io"
            - group: "apps"
            - group: "authentication.k8s.io"
            - group: "authorization.k8s.io"
            - group: "autoscaling"
            - group: "batch"
            - group: "certificates.k8s.io"
            - group: "extensions"
            - group: "metrics.k8s.io"
            - group: "networking.k8s.io"
            - group: "policy"
            - group: "rbac.authorization.k8s.io"
            - group: "scheduling.k8s.io"
            - group: "settings.k8s.io"
            - group: "storage.k8s.io"
            omitStages:
            - "RequestReceived"
          # Default level for all other requests.
          - level: Metadata
            omitStages:
            - "RequestReceived"
        owner: root:root
        path: /etc/kubernetes/audit-policy.yaml
    users:
    - name: tink-user
      sshAuthorizedKeys:
      - 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC1BK73XhIzjX+meUr7pIYh6RHbvI3tmHeQIXY5lv7aztN1UoX+bhPo3dwo2sfSQn5kuxgQdnxIZ/CTzy0p0GkEYVv3gwspCeurjmu0XmrdmaSGcGxCEWT/65NtvYrQtUE5ELxJ+N/aeZNlK2B7IWANnw/82913asXH4VksV1NYNduP0o1/G4XcwLLSyVFB078q/oEnmvdNIoS61j4/o36HVtENJgYr0idcBvwJdvcGxGnPaqOhx477t+kfJAa5n5dSA5wilIaoXH5i1Tf/HsTCM52L+iNCARvQzJYZhzbWI1MDQwzILtIBEQCJsl2XSqIupleY8CxqQ6jCXt2mhae+wPc3YmbO5rFvr2/EvC57kh3yDs1Nsuj8KOvD78KeeujbR8n8pScm3WDp62HFQ8lEKNdeRNj6kB8WnuaJvPnyZfvzOhwG65/9w13IBl7B1sWxbFnq2rMpm5uHVK7mAmjL0Tt8zoDhcE1YJEnp9xte3/pvmKPkST5Q/9ZtR9P5sI+02jY0fvPkPyC03j2gsPixG7rpOCwpOdbny4dcj0TDeeXJX8er+oVfJuLYz0pNWJcT2raDdFfcqvYA0B0IyNYlj5nWX4RuEcyT3qocLReWPnZojetvAG/H8XwOh7fEVGqHAKOVSnPXCSQJPl6s0H12jPJBDJMTydtYPEszl4/CeQ=='
      sudo: ALL=(ALL) NOPASSWD:ALL
    format: cloud-config
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: TinkerbellMachineTemplate
      name: test-control-plane-template-1234567890000
  replicas: 1
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 1
  version: v1.21.2-eks-1-21-4
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: TinkerbellMachineTemplate
metadata:
  name: test-control-plane-template-1234567890000
  namespace: eksa-system
spec:
  template:
    spec:
      hardwareAffinity:
        required:
        - labelSelector:
            matchLabels: 
              type: cp
      templateOverride: |
        global_timeout: 6000
        id: ""
        name: tink-test
        tasks:
        - actions:
          - environment:
              COMPRESSED: "true"
              DEST_DISK: /dev/sda
              IMG_URL: ""
            image: image2disk:v1.0.0
            name: stream-image
            timeout: 360
          - environment:
              BLOCK_DEVICE: /dev/sda2
              CHROOT: "y"
              CMD_LINE: apt -y update && apt -y install openssl
              DEFAULT_INTERPRETER: /bin/sh -c
              FS_TYPE: ext4
            image: cexec:v1.0.0
            name: install-openssl
            timeout: 90
          - environment:
              CONTENTS: |
                network:
                  version: 2
                  renderer: networkd
                  ethernets:
                      eno1:
                          dhcp4: true
                      eno2:
                          dhcp4: true
                      eno3:
                          dhcp4: true
                      eno4:
                          dhcp4: true
              DEST_DISK: /dev/sda2
              DEST_PATH: /etc/netplan/config.yaml
              DIRMODE: "0755"
              FS_TYPE: ext4
              GID: "0"
              MODE: "0644"
              UID: "0"
            image: writefile:v1.0.0
            name: write-netplan
            timeout: 90
          - environment:
              CONTENTS: |
                datasource:
                  Ec2:
                    metadata_urls: []
                    strict_id: false
                system_info:
                  default_user:
                    name: tink
                    groups: [wheel, adm]
                    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
                    shell: /bin/bash
                manage_etc_hosts: localhost
                warnings:
                  dsid_missing_source: off
              DEST_DISK: /dev/sda2
              DEST_PATH: /etc/cloud/cloud.cfg.d/10_tinkerbell.cfg
              DIRMODE: "0700"
              FS_TYPE: ext4
              GID: "0"
              MODE: "0600"
            image: writefile:v1.0.0
            name: add-tink-cloud-init-config
            timeout: 90
          - environment:
              CONTENTS: |
                datasource: Ec2
              DEST_DISK: /dev/sda2
              DEST_PATH: /etc/cloud/ds-identify.cfg
              DIRMODE: "0700"
              FS_TYPE: ext4
              GID: "0"
              MODE: "0600"
              UID: "0"
            image: writefile:v1.0.0
            name: add-tink-cloud-init-ds-config
            timeout: 90
          - environment:
              BLOCK_DEVICE: /dev/sda2
              FS_TYPE: ext4
            image: kexec:v1.0.0
            name: kexec-image
            pid: host
            timeout: 90
          name: tink-test
          volumes:
          - /dev:/dev
          - /dev/console:/dev/console
          - /lib/firmware:/lib/firmware:ro
          worker: '{{.device_1}}'
        version: "0.1"
        
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: TinkerbellCluster
metadata:
  name:  test
  namespace: eksa-system
spec:
  imageLookupFormat: --kube-v1.21.2-eks-1-21-4.raw.gz
  imageLookupBaseRegistry: /